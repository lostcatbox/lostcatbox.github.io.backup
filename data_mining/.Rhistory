names(data)
str(data)
summary(data)
summary(data$행정구역.시군구.별)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국",]
head(data2)
View(data2)
str(data2)
View(data2)
head(data2)
#"시" 단위 지역 통계 삭제
x <- grep("시$", data2$행정구역.시군구.별) # grep()함수는 어떤 문자열이 어디에위치하는지(포함되어있어도) 알고싶을때. 텍스트 검색
x
data3 <- data2[-c(x),] #x를 제외한 !
View(data3)
#순이동 인구수(전출보다 전입 인구수)가 많은 지역
data4 <- data3[1,2]
data4
View(data3)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국",c(1,2,3,4)]
head(data2)
View(data2)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국",c(1,2,3,4),4]
head(data2)
View(data2)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국",c(1,2,3,4)]
View(data2)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국",2]
head(data2)
View(data)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국",2, 10]
head(data2)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국",2]
head(data2)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국"]
head(data2)
View(data2)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국",]
head(data2)
View(data2)
#"전국" 지역 통계 삭제
data2 <- data[data$행정구역.시군구.별 != "전국",]
head(data2)
#"시" 단위 지역 통계 삭제
x <- grep("시$", data2$행정구역.시군구.별) # grep()함수는 어떤 문자열이 어디에위치하는지(포함되어있어도) 알고싶을때. 텍스트 검색
x
data3 <- data2[-c(x),] #x를 제외한 !
#순이동 인구수(전출보다 전입 인구수)가 많은 지역
data4 <- data3[1,2]
data4
#순이동 인구수(전출보다 전입 인구수)가 많은 지역
data4 <- data3[data3$순이동..명.>0]
data4
#순이동 인구수(전출보다 전입 인구수)가 많은 지역
data4 <- data3[data3$순이동..명.>0, ]
data4
View(data4)
#순이동 인구수(전출보다 전입 인구수)가 많은 지역
data4 <- data3[data3$순이동..명.>0, 2]
data4
#순이동 인구수(전출보다 전입 인구수)가 많은 지역
data4 <- data3[data3$순이동..명.>0, 1]
data4
#순이동 인구수(전출보다 전입 인구수)가 많은 지역
data4 <- data3[data3$순이동..명.>0, ]
data4
word <- data4$행정구역.시군구.별
word
frequency <- data4$순이동..명.
frequency
wordcloud(word, frequency, colors=pal2)
#순이동 인구수(전입보다 전출 인구수)가 많은 지역
data5 <- data3[data3$순이동..명.<0, ]
data5
word <- data5$행정구역.시군구.별
word
frequency <- data5$순이동..명.
frequency
wordcloud(word, frequency, colors=pal2)
#순이동 인구수(전입보다 전출 인구수)가 많은 지역
data5 <- data3[data3$순이동..명.<0, ]
data5
#순이동 인구수(전출보다 전입 인구수)가 많은 지역 ,다음수를 넣으면 행!
data4 <- data3[data3$순이동..명.>0, ]
data4
#순이동 인구수(전출보다 전입 인구수)가 많은 지역 ,다음수를 넣으면 행!
data4 <- data3[data3$순이동..명.>0, ]
data4
#순이동 인구수(전입보다 전출 인구수)가 많은 지역
data5 <- data3[data3$순이동..명.<0, ]
data5
word <- data5$행정구역.시군구.별
word
frequency <- data5$순이동..명.
frequency
wordcloud(word, frequency, colors=pal2)
wordcloud(word, frequency, colors=pal2)
wordcloud(word, frequency)
#순이동 인구수(전입보다 전출 인구수)가 많은 지역
data5 <- data3[data3$순이동..명.<0, ]
data5
word <- data5$행정구역.시군구.별
word
frequency <- abs(data5$순이동..명.)
frequency
wordcloud(word, frequency, colors=pal2)
wordcloud(word, frequency)
#순이동 인구수(전입보다 전출 인구수)가 많은 지역
data5 <- data3[data3$순이동..명.<0, ]
data5
word <- data5$행정구역.시군구.별
word
frequency <- abs(data5$순이동..명.) #-값이므로 절댓값
frequency
wordcloud(word, frequency, colors=pal2)
print(word)
# Install
install.packages("tm")           # for text mining
install.packages("SnowballC")    # for text stemming
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
# Read the text file from internet
filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
text <- readLines(filePath) #txt파일 읽기
text
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
docs
# Inspet documents
inspect(docs)
# Inspect documents
inspect(docs)
# Text transformation
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Cleaning text
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Text transformation
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs
# Text transformation
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Cleaning text
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
# Build data matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# Generate wordcloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
z <- matrix(1:20, nrow=4, ncol=5)
z
z[2,1:3]
z[1:2,]
z[,c(1,4)]
install.packages("BostonHousing")
install.packages("mlbench")
data("BostonHousing")
data("BostonHousing")
install.packages("BostonHousing")
install.packages("BostonHousings")
install.packages("devtools")
require(devtools)
install_github("BostonHousing")
library(mlbench)
data("BostonHousing")
library(mlbench)
data("BostonHousing")
myds <- BostonHousing[, c("crim","rm","dis","tax","medv")]
library(mlbench)
data("BostonHousing")
myds <- BostonHousing[, c("crim","rm","dis","tax","medv")]
grp <- c()
for (i in 1:nrow(myds)) {
if (myds$medv[i] >= 25.0) {
grp[i] <- "H"
} else if (myds$medv[i] <= 17.0) {
grp[i] <- "L"
} else {
grp[i] <- "M"
}
}
grp <- factor(grp)        #문자 벡터를 팩터 타입으로 변경
grp <- factor(grp, levels=c("H","M","L")) #레벨의 순서를 H,L,M>>H,M,L순으로 변경
myds <- data.frame(myds, grp) #myds에 grp 열 추가
str(myds)
head(myds)
table(myds$grp)
#히스토그램에 의한 관측값의 분포확인
par(mfrow=c(2,3))
for(i in 1:5){
hist(myds[,i], main=colnames(myds)[i], col="yellow")
}
#상자 그림
par(mfrow=c(2,3))
for(i in 1:5){
boxplot(myds[,i], main=colnames(myds)[i])
}
#그룹별 관측값 분포의확인
boxplot(myds$crim~myds$grp, main="1인당 범죄율")
boxplot(myds$rm~myds$grp, main="방의 갯수")
#다중 산점도를 통한 변수간 상관 관계확인
pairs(myds[,-6])
#그룹 정보를 포함한
point <- as.integer(myds$grp) #점의 모양 지정
color <- c("red","blue","green") #점의 색 지정
pairs(myds[,-6], pch=point, col=color[point])
#변수간의 상관계수의 확인
cor(myds[,-6])
#복습
## (1) Prepare Data ----------------------
library(mlbench)
data("BostonHousing")
myds <- BostonHousing[,c("crim","rm","dis","tax","medv")]
## (2) Add new column ----------------------
grp <- c()
for (i in 1:nrow(myds)) {           # myds$medv 값에 따라 그룹 분류
if (myds$medv[i] >= 25.0) {
grp[i] <- "H"
} else if (myds$medv[i] <= 17.0) {
grp[i] <- "L"
} else {
grp[i] <- "M"
}
}
grp <- factor(grp) # 문자벡터를 팩터 타입으로 변경
grp <- factor(grp, levels=c("H","M","L")) # 레벨의 순서를 H,L,M -> H,M,L
myds <- data.frame(myds, grp) # myds 에 grp 컬럼추가
## (3) Add new column ----------------------
str(myds)
head(myds)
table(myds$grp) # 주택 가격 그룹별 분포
## (4) histogram ----------------------
par(mfrow=c(2,3)) # 2x3 가상화면 분할
for(i in 1:5) {
hist(myds[,i], main=colnames(myds)[i], col="yellow")
}
par(mfrow=c(1,1)) # 2x3 가상화면 분할 해제
## (5) boxplot ----------------------
par(mfrow=c(2,3)) # 2x3 가상화면 분할
for(i in 1:5) {
boxplot(myds[,i], main=colnames(myds)[i])
}
par(mfrow=c(1,1)) # 2x3 가상화면 분할 해제
## (6) boxplot by group ------------------
boxplot(myds$crim~myds$grp, main="1인당 범죄율")
boxplot(myds$rm~myds$grp, main="방의 수")
boxplot(myds$dis~myds$grp, main="직업센터까지의 거리")
boxplot(myds$tax~myds$grp, main="제산세")
## (7) scatter plot ------------------
pairs(myds[,-6])
## (8) scatter plot with group ------------------
point <- as.integer(myds$grp) # 점의 모양 지정
color <- c("red","green","blue") # 점의 색 지정
pairs(myds[,-6], pch=point, col=color[point])
## (9) correlation coefficient ------------------
cor(myds[,-6])
getwd()
setwd("/Users/lostcatbox/univ_lecture/data_mining/")
data <- read.csv("Lec14_neuralnetwork.csv", header = T)
str(data)
# Train set(6)과 Test set(4)으로 나누기
sep <- sample(2, nrow(data), prob = c(0.6,0.4), replace=T)
train <- data[sep==1,]
test <- data[sep==2,]
# 다음 조합 데이터 프레임 생성 가중치 감소가 0.3, 5e-4 -> decay 은닉노드 수가 1, 2, 3 -> size
exp <- expand.grid(decay=c(0.3,5e=4), size=c(1,2,3))
exp
# 다음 조합 데이터 프레임 생성 가중치 감소가 0.3, 5e-4 -> decay 은닉노드 수가 1, 2, 3 -> size
exp <- expand.grid(decay=c(0.3,5e-4), size=c(1,2,3))
exp
exp
install.packages("caret")
library(caret)
library(caret)
#  Train set으로 모형 만들기 -> maxit = 200  , train 함수는 패키지caret필요
fit <-  train(share~., data=train, method="nnet", maxit=200, tuneGrid=exp, trace=F, lnear.output=T)
#  Train set으로 모형 만들기 -> maxit = 200  , train 함수는 패키지caret필요
fit <-  train(share~., data=train, method="nnet", maxit=200, tuneGrid=exp, trace=F, lnear.output=T)
# 최적 가중치 감소 값과 은닉노드의 수 구하고 그래프 확인
fit$bestTune
plot(fit)
library(ModelMetrics)
getwd()
# Train set으로 모형 예측 및 mse 확인(이는 필수과정아님,회귀분석과 비교할려고)
pred <- predict(fit, newdata=train)
mse()#ModelMetrics패키지 사용
mse(train$share, pred)#ModelMetrics패키지 사용
# Test set으로 모형 예측 및 mse 확인
pred <- predict(fit, newdata=test)
mse(train$share, pred)
# Test set으로 모형 예측 및 mse 확인
pred1 <- predict(fit, newdata=test)
mse(train$share, pred1) # 0.016
# Test set으로 모형 예측 및 mse 확인
pred1 <- predict(fit, newdata=test)
mse(train$share, pred1) # 0.016
# Test set으로 모형 예측 및 mse 확인
pred1 <- predict(fit, newdata=test)
mse(train$share, pred1) # 0.016
# Test set으로 모형 예측 및 mse 확인
pred1 <- predict(fit, newdata=test)
mse(train$share, pred1) # 0.016
# Train set으로 모형 예측 및 mse 확인(이는 필수과정아님,회귀분석과 비교할려고)
pred <- predict(fit, newdata=train)
mse(train$share, pred)#ModelMetrics패키지 사용, 0.00017
# Test set으로 모형 예측 및 mse 확인
pred1 <- predict(fit, newdata=test)
mse(train$share, pred1) # 0.016
# Test set으로 모형 예측 및 mse 확인
pred1 <- predict(fit, newdata=test)
mse(train$share, pred1) # 0.016
mse(train$share, pred1) # 0.016
# Test set으로 모형 예측 및 mse 확인
pred1 <- predict(fit, newdata=test)
mse(test$share, pred1) # 0.016
# 실제 값과 예측 값 묶어서 첫 6줄 확인
head(cbind(test@share, pred1))
# 실제 값과 예측 값 묶어서 첫 6줄 확인
head(cbind(test@share, pred1))
# 실제 값과 예측 값 묶어서 첫 6줄 확인
cbind(test@share, pred1)
# 실제 값과 예측 값 묶어서 첫 6줄 확인
cbind(test@share, pred1))
# 실제 값과 예측 값 묶어서 첫 6줄 확인
cbind((test@share, pred1))
# 실제 값과 예측 값 묶어서 첫 6줄 확인
cbind(test@share, pred1)
test$share
# 실제 값과 예측 값 묶어서 첫 6줄 확인
head(cbind(test@share, pred1),1)
# 실제 값과 예측 값 묶어서 첫 6줄 확인
cbind(test@share, pred1)
# 실제 값과 예측 값 묶어서 첫 6줄 확인
x <- cbind(test@share, pred1)
# 실제 값과 예측 값 묶어서 첫 6줄 확인
cbind(test@share, pred1)
# 실제 값과 예측 값 묶어서 첫 6줄 확인
head(cbind(test@share, pred1))
# 실제 값과 예측 값 묶어서 첫 6줄 확인
head(cbind(test@share, pred1))
# Train set으로 회귀분석 돌려보기
fit_1 <- lm(share~., data=train)
fit_2 <- step(fit_1, trace=F)
pred2 <- predict(fit_2, newdata=train)
mse(train$share, pred2)
pred3 <- predict((fit_2, newdata=test))
mse(test$share, pred3)
pred3 <- predict((fit_2, newdata=test))
pred3 <- predict(fit_2, newdata=test)
mse(test$share, pred3)
# 신경망 모형 시각화 하기
source('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1
a4684a5/nnet_plot_update.r')
nn<-summary(fit)
plot(nn)
# 신경망 모형 시각화 하기
source_url("https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r")
nn<-summary(fit)
plot(nn)
# 균등분포로부터 난수 50개 생성(runif(n, min, max) 함수)하여 x에 저장
x <- runif(50, 0, 100)
#생성한 난수의 제곱근(sqrt()함수)을 y로 저장
y <- sqrt(x)
#x와 y를 열결합하여 train에 저장
train <- cbind(x,y)
install.packages("neuralnet")
library(neuralnet)
print(fit)
#neuralnet 함수를 이용하여 신경망 모형 만들기
fit <- neuralnet(y~x, data=train, hidden=10, threshold = 0.01)
print(fit)
#neuralnet 함수를 이용하여 신경망 모형 만들기
fit <- neuralnet(y~x, data=train, hidden=10, threshold = 0.01)
print(fit)
plot(fit)#결과 시각화하기
#1~10 정수 생성하여 y_test에 저장
y_test <- 1:10
#y_test를 제곱하여 data frame 형태로 x_test에 저장 (as.data.frame() 함수 사용)
x_test <- as.data.frame(y_test^2)
#x_test 이용하여 예측
predict <- compute(fit, x_test)
print(predict$net.result)
#x_test 이용하여 예측
predict <- compute(fit, x_test)
print(predict$net.result)
output <- cbind(x_test, y_test, as.data.frame(predict$net.result))
colnames(output) <- c("Input",“Actual Output","Neural Net Output")
#예측 값 출력
#예측 결과 테이블 형태로 변형하여 출력
colnames(output) <- c("Input","Actual Output","Neural Net Output")
print(output)
mse(y_test, predict$net.result)
data <- iris
nn <- neuralnet(species~., data=train, hidden=5, threshold = 0.01, linear.output = F) #분류니까 F
nn <- neuralnet(Species~., data=train, hidden=5, threshold = 0.01, linear.output = F) #분류니까 F
#Train set(5)과 Test set(5)으로 나눈 후, neuralnet 함수를 이용하여 신경망 모형을 만드시오.
data <- iris
sep <- sample(2, nrow(data), prob=c(0.5,0.5), replace=T)
train <- data[sep==1,]
test <- data[sep==2,]
nn <- neuralnet(Species~., data=train, hidden=5, threshold = 0.01, linear.output = F) #분류니까 F
plot(nn)
plot(nn)
plot(nn)
nn <- neuralnet(Species~., data=train, hidden=5, threshold = 0.01, linear.output = F) #분류니까 F
plot(nn)
#Train set(5)과 Test set(5)으로 나눈 후, neuralnet 함수를 이용하여 신경망 모형을 만드시오.
data <- iris
View(nn)
View(data)
nn <- neuralnet(Species~., data=train, hidden=5, threshold = 0.01, linear.output = F) #분류니까 F
plot(nn)
nn <- neuralnet(Species~., data=train, hidden=5, threshold = 0.01, linear.output = F) #분류니까 F
plot(nn)
p <- compute(nn, test[,1:4])
print(p$net.resulp)
print(p$net.result)
p2 <- ifelse(p1>=0.05,1.0)
p1 <- print(p$net.result)
p2 <- ifelse(p1>=0.05,1.0)
p <- compute(nn, test[,1:4])
p1 <- print(p$net.result)
p2 <- ifelse(p1>=0.05,1.0)
p2 <- ifelse(p1>=0.05,1,0)
p2
p3 <- max.col(p2)
p3
k <- table(test$Species, p3)
k
k <- table(test$Species, p3)
k
#오분류를 최소화하는 은닉노드 수를 그래프를 통하여 구하시오.
error <- sum(k[1,-1], k[2,-2], k[3,-3])/sum(k) #오차는 table에서 대각선을 제외한것이므로(해당-부호의뜻)
error
#오분류를 최소화하는 은닉노드 수를 그래프를 통하여 구하시오.
test.error <- function(hiddensize){           #함수
nn <- neuralnet(Y~., data=traindata, hidden=hiddensize, threshold=0.01, linear.output=F)
p<- compute(nn, test[,1:4])    #꼭 종속변수는 제외시켜줘
p1<-print(p$net.result)
p2<-ifelse(p1>=0.5,1,0)
p3<-max.col(p2)
k<-table(test$Species,p3)
error <- sum(k[1,-1],k[2,-2],k[3,-3])/sum(k)         #오분류 수 / 전체 수
c(hiddensize,error)
}
out<-t(sapply(2:10,FUN=test.error)) #test.error라는 함수를 이용하여 은닉 노드수(2~10개)에 따른 오분류율 계산
plot(out, type="b", xlab="the number of hidden units", ylab="test error") #오분류율 그래프 출력
out<-t(sapply(2:10,FUN=test.error)) #test.error라는 함수를 이용하여 은닉 노드수(2~10개)에 따른 오분류율 계산
p<- compute(nn, test[,1:4])    #꼭 종속변수는 제외시켜줘
p1<-print(p$net.result)
p2<-ifelse(p1>=0.5,1,0)
p3<-max.col(p2)
k<-table(test$Species,p3)
error <- sum(k[1,-1],k[2,-2],k[3,-3])/sum(k)         #오분류 수 / 전체 수
c(hiddensize,error)
#오분류를 최소화하는 은닉노드 수를 그래프를 통하여 구하시오.
test.error <- function(hiddensize){           #함수
nn <- neuralnet(Species~., data=train, hidden=hiddensize, threshold=0.01, linear.output=F)
p<- compute(nn, test[,1:4])    #꼭 종속변수는 제외시켜줘
p1<-print(p$net.result)
p2<-ifelse(p1>=0.5,1,0)
p3<-max.col(p2)
k<-table(test$Species,p3)
error <- sum(k[1,-1],k[2,-2],k[3,-3])/sum(k)         #오분류 수 / 전체 수
c(hiddensize,error)
}
out<-t(sapply(2:10,FUN=test.error)) #test.error라는 함수를 이용하여 은닉 노드수(2~10개)에 따른 오분류율 계산
plot(out, type="b", xlab="the number of hidden units", ylab="test error") #오분류율 그래프 출력
out
